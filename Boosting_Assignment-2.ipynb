{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Q1. Gradient Boosting Regression is a machine learning technique used for regression tasks. It's an ensemble method where multiple weak learners (usually decision trees) are combined sequentially to create a strong learner. In each iteration, a new model is trained to correct the errors made by the previous models. Gradient boosting optimizes a loss function using gradient descent, where the gradients are computed from the loss function with respect to the predictions of the ensemble.\n",
    "\n",
    "Q2. Sure, I can walk you through implementing a simple gradient boosting algorithm from scratch in Python using NumPy for a regression problem. Here's a basic outline of the algorithm:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = np.copy(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.models.append(tree)\n",
    "            # Update residuals\n",
    "            residuals -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "This code implements a simple gradient boosting regressor using decision trees as the base learner.\n",
    "\n",
    "Q3. To optimize the performance of the model, you can experiment with different hyperparameters such as learning rate, number of trees, and tree depth. You can use grid search or random search techniques to find the best hyperparameters.\n",
    "\n",
    "Q4. In Gradient Boosting, a weak learner is a model that performs slightly better than random guessing. In the context of decision trees, weak learners are typically shallow trees, meaning they have limited depth and are not highly expressive on their own.\n",
    "\n",
    "Q5. The intuition behind the Gradient Boosting algorithm is to sequentially add predictors to an ensemble, where each predictor corrects the errors made by its predecessor. It's like a team of experts who improve their performance over time by learning from their mistakes.\n",
    "\n",
    "Q6. Gradient Boosting builds an ensemble of weak learners sequentially. At each iteration, a new weak learner is trained to correct the errors made by the existing ensemble. The final prediction is the sum of predictions from all weak learners, weighted by a learning rate.\n",
    "\n",
    "Q7. The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm include:\n",
    "\n",
    "Initialize the model with a constant value.\n",
    "Fit a weak learner to the residuals of the current model.\n",
    "Update the model by adding the weak learner's prediction scaled by a learning rate.\n",
    "Repeat steps 2-3 until a stopping criterion is met, such as reaching a maximum number of iterations or achieving satisfactory performance.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
